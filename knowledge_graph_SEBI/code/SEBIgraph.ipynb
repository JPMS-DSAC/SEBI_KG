{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import bs4\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Issue and Listing of Non-Convertible Redeemable Preference Shares\", \"Investment Advisers\", \"Depositories and Participants\", \"Mutual Funds\", \"Employees Service\", \"Substantial Acquisition of Shares and Takeovers\", \"Appointment of Administrator and Procedure for Refunding to the Investors\", \"Prohibition of Fraudulent and Unfair Trade Practices relating to Securities Market\", \"Know Your Client\", \"Prohibition of Insider Trading\", \"Merchant Bankers\", \"Issue and Listing of Securitised Debt Instruments and Security Receipts\", \"Delisting of Equity Shares\", \"Issue of Capital and Disclosure Requirements\", \"Foreign Venture Capital Investor\", \"Procedure for Board Meetings\", \"Custodian\", \"Ombudsman\", \"Investor Protection and Education Fund\", \"Foreign Portfolio Investors\", \"Issue of Sweat Equity\", \"Collective Investment Scheme\", \"Portfolio Managers\", \"Research Analysts\", \"Procedure for Search and Seizure\", \"Issue Of Capital And Disclosure Requirements\", \"Share Based Employee Benefits\", \"Debenture Trustees\", \"Alternative Investment Funds\", \"Stock Exchanges and Clearing Corporations\", \"Self Regulatory Organisations\", \"Settlement Proceedings\", \"Issue and Listing of Municipal Debt Securities\", \"Buy-back of Securities\", \"Issue and Listing of Debt Securities\", \"Infrastructure Investment Trusts\", \"Stock Brokers\", \"Listing Obligations and Disclosure Requirements\", \"Registrars to an Issue and Share Transfer Agents\", \"Real Estate Investment Trusts\", \"Intermediaries\", \"Certification of Associated Persons in the Securities Markets\", \"Credit Rating Agencies\", \"Regulatory Fee on Stock Exchanges\", \"Underwriters\", \"Buy Back Of Securities\", \"Bankers to an Issue\", \"Central Database of Market Participants\"]\n",
    "with open('cleanedregulations48.pkl','rb') as f:\n",
    "    finalclean48 = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting cross-references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regids(k):\n",
    "    regind = []\n",
    "    for i in k:\n",
    "        p = i.split()[0]\n",
    "        if('.' in p):\n",
    "            if(len(p) < 4):\n",
    "                l = p.replace('.','')\n",
    "                regind.append([l,'1'])\n",
    "            else:\n",
    "                r = regind[-1]\n",
    "                r1 = int(r[1]) + 1\n",
    "                regind.append([l,str(r1)])\n",
    "        else:\n",
    "            t=0\n",
    "            while(t<len(p)):\n",
    "                if(p[t] == '('):\n",
    "                    start = t\n",
    "                if(p[t] == ')'):\n",
    "                    break\n",
    "                t+=1\n",
    "            start = start+1\n",
    "            sind = ''\n",
    "            while(start<t):\n",
    "                sind = sind + p[start]\n",
    "                start+=1\n",
    "        \n",
    "            regind.append([l,sind])\n",
    "    return regind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossref(k, regind, reggraphdata):\n",
    "    total=0\n",
    "    for i in k:\n",
    "        totalreg = []\n",
    "        refregs = [] # [subreg id, reg id]\n",
    "        if('sub-regulation (' in i):\n",
    "            p = i.split()\n",
    "            t=0\n",
    "            start = []\n",
    "            end = []\n",
    "            totalreg = []\n",
    "            while(t<len(p)):\n",
    "                if(p[t]=='sub-regulation'):\n",
    "                    if(len(start) != len(end)):\n",
    "                        end.append('nil')\n",
    "                    start.append(t)\n",
    "                if(p[t]=='regulation'):\n",
    "                    if(len(start) == len(end)+1):\n",
    "                        end.append(t)\n",
    "                    else:\n",
    "                        if(t < len(p)-1):\n",
    "                            cleanreg = p[t+1]\n",
    "                            if('.' in p[t+1]):\n",
    "                                cleanreg = p[t+1].replace('.','')\n",
    "                            if(',' in p[t+1]):\n",
    "                                cleanreg = p[t+1].replace(',','')\n",
    "                            if(cleanreg not in totalreg):\n",
    "                                totalreg.append(cleanreg)\n",
    "                if(len(start) !=0):\n",
    "                    if(t < len(p)-5 and t!=len(p)-1):\n",
    "                        if(t == start[-1]+4):\n",
    "                            if(len(start) != len(end)):\n",
    "                                end.append('nil')\n",
    "                    else:\n",
    "                        if(t == len(p)-1):\n",
    "                            if(len(start) != len(end)):\n",
    "                                end.append('nil')\n",
    "                t+=1\n",
    "            '''\n",
    "            print(start)\n",
    "            print(end)\n",
    "            print(len(p))\n",
    "            print(i)\n",
    "            print('-----------------')\n",
    "            '''\n",
    "            \n",
    "            itert=0\n",
    "            while(itert<len(start) and start[itert]<len(p)-1):\n",
    "                m = p[start[itert]+1]\n",
    "            \n",
    "                t=0\n",
    "                while(t<len(m)):\n",
    "                    if(m[t] == '('):\n",
    "                        mstart = t\n",
    "                    if(m[t] == ')'):\n",
    "                        break\n",
    "                    t+=1\n",
    "                mstart = mstart+1\n",
    "                sind = ''\n",
    "                while(mstart<t):\n",
    "                    sind = sind + m[mstart]\n",
    "                    mstart+=1\n",
    "                \n",
    "                if(end[itert] != 'nil'):\n",
    "                    if(end[itert] < len(p)-1):\n",
    "                        cleanreg = p[end[itert]+1]\n",
    "                        if('.' in p[end[itert]+1]):\n",
    "                            cleanreg = p[end[itert]+1].replace('.','')\n",
    "                        if(',' in p[end[itert]+1]):\n",
    "                            cleanreg = p[end[itert]+1].replace(',','')\n",
    "                        if([sind,cleanreg] not in refregs):\n",
    "                            refregs.append([cleanreg,sind])\n",
    "                            if(cleanreg in totalreg):\n",
    "                                totalreg.remove(cleanreg)\n",
    "                    else:\n",
    "                        start.pop(itert)\n",
    "                else:\n",
    "                    r = regind[total]\n",
    "                    if([r[0],sind] not in refregs):\n",
    "                        refregs.append([r[0],sind])\n",
    "                itert+=1\n",
    "                \n",
    "            for j in totalreg:\n",
    "                riter=0\n",
    "                for r in regind:\n",
    "                    if(j in r[0]):\n",
    "                        if([k[riter],i, '0'] not in reggraphdata):\n",
    "                            reggraphdata.append([k[riter],i,'0'])\n",
    "                    riter = riter+1\n",
    "            \n",
    "            for refdata in refregs:\n",
    "                if(refdata in regind):\n",
    "                    sourceind = regind.index(refdata)\n",
    "                    if([k[sourceind],i, '0'] not in reggraphdata):\n",
    "                        reggraphdata.append([k[sourceind],i, '0'])\n",
    "        total = total+1\n",
    "    return reggraphdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reggraphdata = []\n",
    "t=0\n",
    "for k in finalclean48:\n",
    "    regind = regids(k)\n",
    "    reggraphdata = crossref(k, regind, reggraphdata)\n",
    "    t=t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs_chaps.pkl','rb') as f:\n",
    "    docchaps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapterconnect(k, d, reggraphdata):\n",
    "    \n",
    "    indlist = []\n",
    "    t=1\n",
    "    p=t\n",
    "    while(t<len(d)):\n",
    "        if(d[t] != d[p]):\n",
    "            p=t\n",
    "            indlist.append(t-1)\n",
    "        if(t==len(d)-1):\n",
    "            indlist.append(t)\n",
    "        t+=1\n",
    "    \n",
    "    t=0\n",
    "    for i in indlist:\n",
    "        while(t<i):\n",
    "            if([k[t],k[t+1],'0'] not in reggraphdata and [k[t],k[t+1],'1'] not in reggraphdata):\n",
    "                reggraphdata.append([k[t],k[t+1],'1'])\n",
    "            t+=1\n",
    "    \n",
    "    return reggraphdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "citer=0\n",
    "for k in finalclean48:\n",
    "    d = docchaps[citer]\n",
    "    reggraphdata = chapterconnect(k, d, reggraphdata)\n",
    "    citer = citer+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reg_topics.pkl','rb') as f:\n",
    "    regtopics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulations = []\n",
    "for i in finalclean48:\n",
    "    regulations.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commontopics(regtopics,regulations,reggraphdata):\n",
    "    t=0\n",
    "    while(t<len(regtopics)):\n",
    "        p=t+1\n",
    "        while(p<len(regtopics)):\n",
    "            for i in regtopics[t]:\n",
    "                if(i in regtopics[p]):\n",
    "                    if([regulations[t],regulations[p],'0'] not in reggraphdata and [regulations[t],regulations[p],'1'] not in reggraphdata and [regulations[t],regulations[p],'2'] not in reggraphdata):\n",
    "                        reggraphdata.append([regulations[t],regulations[p],'2'])\n",
    "                        break\n",
    "            p+=1\n",
    "        t+=1\n",
    "    return reggraphdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-813b05fc979a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreggraphdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommontopics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregtopics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreggraphdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c52b4cb37864>\u001b[0m in \u001b[0;36mcommontopics\u001b[0;34m(regtopics, regulations, reggraphdata)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreggraphdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreggraphdata\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreggraphdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                         \u001b[0mreggraphdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reggraphdata = commontopics(regtopics,regulations,reggraphdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('reggraphdata.pkl', 'wb') as f:\n",
    "    pickle.dump(reggraphdata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1(df,h):\n",
    "    sources = df['source']\n",
    "    targets = df['target']\n",
    "    labels = df['label']\n",
    "    q = 'Layer 1 - Regulatory Documents'\n",
    "    edge_data = zip(sources, targets, labels)\n",
    "    \n",
    "    #nt = Network(height=\"750px\", width=\"100%\", heading=q)\n",
    "    nt = Network(height=\"100%\", width=\"100%\",heading=q)\n",
    "    nt.hrepulsion()\n",
    "    \n",
    "    t=0\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        lbl = e[2]\n",
    "        \n",
    "        if(h[t] == 'amendment'):\n",
    "            nt.add_node(src, src, title=src, color = \"blue\", size=100, url='./Regdoc_graphs/'+src+'.html')\n",
    "            nt.add_node(dst, label='a', title='Amendment Timeline of '+src, color = \"red\",size=50, url='./amendment_timeline/'+src+'.html')\n",
    "            nt.add_edge(src, dst, value=50, title=lbl, color = \"#000000\", length=700)\n",
    "        else:\n",
    "            nt.add_node(src, src, title=src, color = \"blue\", size=100,url='./Regdoc_graphs/'+src+'.html')\n",
    "            nt.add_node(dst, dst, title=dst, color = \"blue\", size=100,url='./Regdoc_graphs/'+dst+'.html')\n",
    "            nt.add_edge(src, dst, value=50, title=lbl, color = \"#000000\", length=700)\n",
    "        t=t+1\n",
    "    \n",
    "    nt.show_buttons()  \n",
    "    nt.show('layer1.html')\n",
    "    \n",
    "    with open(\"layer1.html\") as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "\n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "\n",
    "    t=0\n",
    "    appendind = []\n",
    "    for i in tokens1:\n",
    "        if('return' in i):\n",
    "            appendind.append(t)\n",
    "        t=t+1\n",
    "    \n",
    "    with open(\"/home/deepti-saravanan/Desktop/laws/interactive.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<appendind[-1]):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = appendind[-1]\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "\n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open(\"layer1.html\", \"w\") as outf:\n",
    "        outf.write(str(soup))\n",
    "        \n",
    "    with open(\"layer1.html\") as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "    \n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "    \n",
    "    t0=0\n",
    "    while(t0<len(tokens1)):\n",
    "        if('\"edges\": {' in tokens1[t0]):\n",
    "            break\n",
    "        t0+=1\n",
    "        \n",
    "    with open(\"/home/deepti-saravanan/Desktop/laws/nodefont.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<t0):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = t0\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "    \n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open(\"layer1.html\", \"w\") as outf:\n",
    "        outf.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerinfo = []\n",
    "h=[]\n",
    "t=0\n",
    "for i in docs:\n",
    "    if(t<len(docs)-1):\n",
    "        layerinfo.append([i,docs[t+1],'Click on the nodes for detailed analysis of regulations'])\n",
    "        h.append('regulations')\n",
    "    layerinfo.append([i,'a'+str(t),'Detailed biography of regulations'])\n",
    "    h.append('amendment')\n",
    "    t=t+1\n",
    "layerinfo.append([docs[-1],'all','Click on the nodes for detailed analysis of regulations'])\n",
    "h.append('regulations')\n",
    "df = pd.DataFrame(layerinfo, columns=['source','target','label'])\n",
    "layer1(df,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2 (Amendments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conceptpaper(docs):\n",
    "    \n",
    "    conreg_dict = {}\n",
    "    for i in docs:\n",
    "        conreg_dict[i.lower()] = i.lower()\n",
    "    conreg_dict['aif'] = 'alternative investment funds'\n",
    "    conreg_dict['aifs'] = 'alternative investment funds'\n",
    "    conreg_dict['secc'] = 'stock exchanges and clearing corporations'\n",
    "    conreg_dict['reit'] = 'real estate investment trusts'\n",
    "    conreg_dict['reits'] = 'real estate investment trusts'\n",
    "    conreg_dict['invit'] = 'infrastructure investment trusts'\n",
    "    conreg_dict['invits'] = 'infrastructure investment trusts'\n",
    "    conreg_dict['pms'] = 'portfolio managers'\n",
    "    conreg_dict['icdr'] = 'issue of capital and disclosure requirements'\n",
    "    conreg_dict['fvci'] = 'foreign venture capital investor'\n",
    "    conreg_dict['fvcis'] = 'foreign venture capital investor'\n",
    "    conreg_dict['fpi'] = 'foreign portfolio investors'\n",
    "    conreg_dict['sast'] = 'substantial acquisition of shares and takeovers'\n",
    "    conreg_dict['ilds'] = 'issue and listing of debt securities'\n",
    "    conreg_dict['takeover regulations'] = 'substantial acquisition of shares and takeovers'\n",
    "    conreg_dict['br'] = 'bankers to an issue'\n",
    "    \n",
    "    regs = list(conreg_dict.keys())\n",
    "    onlyfiles = [f for f in listdir('/home/deepti-saravanan/Documents/Deepti/') if isfile(join('/home/deepti-saravanan/Documents/Deepti/',f))]\n",
    "    \n",
    "    concept_dir = '/home/deepti-saravanan/Documents/Deepti/'\n",
    "    regwords = ['regulation', 'Regulation', 'Regulations', 'regulations']\n",
    "    stopwords = ['background','objective','1.','1.0','/','^','scenario']\n",
    "    symbols = ['(',')',',','.',]\n",
    "    flag=0\n",
    "    final_content = []\n",
    "    for filename in onlyfiles:\n",
    "        reg_doc = []\n",
    "        year = []\n",
    "        with open(concept_dir+filename, encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.readlines()\n",
    "            content = [x.strip() for x in content] \n",
    "        for line in content:\n",
    "            for j in regwords:\n",
    "                if(j in line):\n",
    "                    for symb in symbols:\n",
    "                        line = line.replace(symb,'')\n",
    "                    for regdocname in regs:\n",
    "                        if(regdocname in line.lower()):\n",
    "                            if(conreg_dict[regdocname] not in reg_doc):\n",
    "                                reg_doc.append(conreg_dict[regdocname])\n",
    "                                flag=1\n",
    "                                year.append('2021')\n",
    "                                for sline in line.split():\n",
    "                                    if(sline.isdigit() == True and len(sline) == 4):\n",
    "                                        year[-1] = sline\n",
    "                            else:\n",
    "                                ind = reg_doc.index(conreg_dict[regdocname])\n",
    "                                y = year[ind]\n",
    "                                if(y == '2021'):\n",
    "                                    for sline in line.split():\n",
    "                                        if(sline.isdigit() == True and len(sline) == 4):\n",
    "                                            year[ind] = sline    \n",
    "                                        \n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            c = content[0:3]\n",
    "            s = ''\n",
    "            for i in c:\n",
    "                stopflag=0\n",
    "                for j in stopwords:\n",
    "                    if(j in i.lower()):\n",
    "                        stopflag=1\n",
    "                        break\n",
    "                if(stopflag==0):\n",
    "                    if(len(i.split())<20):\n",
    "                        s = s + ' ' + i\n",
    "            kiter=0\n",
    "            for docnameiter in reg_doc:\n",
    "                if(year[kiter] == '2021'):\n",
    "                    for symb in symbols:\n",
    "                        d = s.replace(symb,'')\n",
    "                    for sline in d.split():\n",
    "                        if(sline.isdigit() == True and len(sline) == 4):\n",
    "                            year[kiter] = sline\n",
    "                        \n",
    "                final_content.append([docnameiter,year[kiter],filename,s])\n",
    "                kiter = kiter+1\n",
    "            s = ''\n",
    "    return final_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_content = conceptpaper(docs)\n",
    "with open('conceptpaper_graph.pkl', 'wb') as f:\n",
    "    pickle.dump(final_content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conceptpaper_graph.pkl', 'rb') as f:\n",
    "    final_content = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linking Amendments Timeline and Concept Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer20(amendfile):\n",
    "    \n",
    "    with open('./amendment/'+amendfile,'rb') as f:\n",
    "        a0 = pickle.load(f)\n",
    "        \n",
    "    adocs = []\n",
    "    for i in a0:\n",
    "        if(i[0] not in adocs):\n",
    "            adocs.append(i[0])\n",
    "        if(i[1] not in adocs):\n",
    "            adocs.append(i[1])\n",
    "            \n",
    "    cplabels = []\n",
    "    for i in cp:\n",
    "        if(i[0] == amendfile.replace('.pkl','').lower()):\n",
    "            for j in adocs:\n",
    "                if(i[1] in j):\n",
    "                    a0.append([j,i[2]])\n",
    "                    cplabels.append(i[3])\n",
    "                    break\n",
    "    a = []\n",
    "    t=0\n",
    "    for i in a0:\n",
    "        if('.txt' in i[1]):\n",
    "            i.extend(['1',cplabels[t]])\n",
    "            t=t+1\n",
    "            a.append(i)\n",
    "        else:\n",
    "            i.extend(['0','amendment'])\n",
    "            a.append(i)\n",
    "    df = pd.DataFrame(a, columns = ['source','target','h','label'])\n",
    "\n",
    "    sources = df['source']\n",
    "    targets = df['target']\n",
    "    labels = df['label']\n",
    "    h = df['h']\n",
    "    q = 'Layer 2 - Amendment Timeline of ' + amendfile.replace('.pkl','')\n",
    "    edge_data = zip(sources, targets, labels,h)\n",
    "    \n",
    "    #nt = Network(height=\"750px\", width=\"100%\", heading=q)\n",
    "    nt = Network(height=\"100%\", width=\"100%\",heading=q)\n",
    "    nt.hrepulsion()\n",
    "    \n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        lbl = e[2]\n",
    "        h = e[3]\n",
    "        \n",
    "        if(h == '0'):\n",
    "            nt.add_node(src, src, title=src, color = \"blue\")\n",
    "            nt.add_node(dst, dst, title=dst, color = \"blue\")\n",
    "            nt.add_edge(src, dst, title=lbl, color = \"#000000\")\n",
    "        else:\n",
    "            nt.add_node(src, src, title=src, color = \"blue\")\n",
    "            nt.add_node(dst, dst, title=dst, color = \"red\",url='file:///home/deepti-saravanan/Documents/reports_for_public_comments/'+dst.replace('.txt','')+'.pdf')\n",
    "            nt.add_edge(src, dst, title=lbl, color = \"#000000\")\n",
    "        t=t+1\n",
    "    \n",
    "    nt.show_buttons()\n",
    "    name = amendfile.replace('.pkl','')\n",
    "    nt.show('./amendment_timeline/'+name+'.html')\n",
    "    \n",
    "    with open('./amendment_timeline/'+name+'.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "\n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "\n",
    "    t=0\n",
    "    appendind = []\n",
    "    for i in tokens1:\n",
    "        if('return' in i):\n",
    "            appendind.append(t)\n",
    "        t=t+1\n",
    "    \n",
    "    with open(\"interactive.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<appendind[-1]):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = appendind[-1]\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "\n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open('./amendment_timeline/'+name+'.html', \"w\") as outf:\n",
    "        outf.write(str(soup))\n",
    "        \n",
    "    with open('./amendment_timeline/'+name+'.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "    \n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "    \n",
    "    t0=0\n",
    "    while(t0<len(tokens1)):\n",
    "        if('\"edges\": {' in tokens1[t0]):\n",
    "            break\n",
    "        t0+=1\n",
    "        \n",
    "    with open(\"nodefont.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<t0):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = t0\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "    \n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open('./amendment_timeline/'+name+'.html', \"w\") as outf:\n",
    "        outf.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "amendfiles = [f for f in listdir('./amendment/') if isfile(join('./amendment/',f))]\n",
    "with open('conceptpaper_graph.pkl','rb') as f:\n",
    "    cp = pickle.load(f)\n",
    "for amendfile in amendfiles:\n",
    "    layer20(amendfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prohibition of Insider Trading'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2 (Reg Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer21(df,idoc, regind, igl, lcl, t, cur_regs):\n",
    "    docnumber = t\n",
    "    sources = df['source']\n",
    "    targets = df['target']\n",
    "    labels = df['label']\n",
    "    q = 'Layer 2 - ' + idoc\n",
    "    edge_data = zip(sources, targets, labels)\n",
    "    \n",
    "    #nt = Network(height=\"750px\", width=\"100%\", heading=q)\n",
    "    nt = Network(height=\"100%\", width=\"100%\",heading=q)\n",
    "    nt.hrepulsion()\n",
    "    \n",
    "    lbl = ['Cross-reference', 'Chapterwise', 'Shared topics']\n",
    "    t=0\n",
    "    igcount=0\n",
    "    lccount=0\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        h = e[2]\n",
    "        if(h == '0'):\n",
    "            sid = cur_regs.index(src)\n",
    "            did = cur_regs.index(dst)\n",
    "            nt.add_node(src, label=str(docnumber)+'_'+str(sid), title=src, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label=str(docnumber)+'_'+str(did), title=dst, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(did)+'.html')\n",
    "            nt.add_edge(src, dst, title=lbl[0], value=8, color = \"#000000\", length=400)\n",
    "        elif(h == '1'):\n",
    "            sid = cur_regs.index(src)\n",
    "            did = cur_regs.index(dst)\n",
    "            nt.add_node(src, label=str(docnumber)+'_'+str(sid), title=src, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label=str(docnumber)+'_'+str(did), title=dst, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(did)+'.html')\n",
    "            nt.add_edge(src, dst, title=lbl[1], value=7, color = \"red\", length=400)\n",
    "        elif(h == '2'):\n",
    "            sid = cur_regs.index(src)\n",
    "            did = cur_regs.index(dst)\n",
    "            nt.add_node(src, label=str(docnumber)+'_'+str(sid), title=src, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label=str(docnumber)+'_'+str(did), title=dst, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(did)+'.html')\n",
    "            nt.add_edge(src, dst, title=lbl[2], color = \"green\", length=400)\n",
    "        elif(h == '3'):\n",
    "            sid = cur_regs.index(src)\n",
    "            nt.add_node(src, label=str(docnumber)+'_'+str(sid), title=src, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label='informal_guidance', title=dst, color = \"red\", url='file:///home/deepti-saravanan/Desktop/knowledge_graph_SEBI/informal_guidance/'+dst.replace('.docx','')+'.pdf')\n",
    "            nt.add_edge(src, dst, title=igl[igcount], color = \"#000000\", length=400)\n",
    "            igcount = igcount+1\n",
    "        else:\n",
    "            sid = cur_regs.index(src)\n",
    "            nt.add_node(src, label=str(docnumber)+'_'+str(sid), title=src, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label='legal_case', title=dst, color = \"black\", url='file:///home/deepti-saravanan/Desktop/knowledge_graph_SEBI/case_pdf/'+dst+'.pdf')\n",
    "            nt.add_edge(src, dst, title=lcl[lccount], color = \"#000000\", length=400)\n",
    "            lccount = lccount+1\n",
    "        t=t+1\n",
    "    \n",
    "    nt.show_buttons()  \n",
    "    nt.write_html('./Regdoc_graphs/'+idoc+'.html')\n",
    "    \n",
    "    with open('./Regdoc_graphs/'+idoc+'.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "\n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "\n",
    "    t=0\n",
    "    appendind = []\n",
    "    for i in tokens1:\n",
    "        if('return' in i):\n",
    "            appendind.append(t)\n",
    "        t=t+1\n",
    "    \n",
    "    with open(\"interactive.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<appendind[-1]):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = appendind[-1]\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "\n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open('./Regdoc_graphs/'+idoc+'.html', \"w\") as outf:\n",
    "        outf.write(str(soup))\n",
    "        \n",
    "    with open('./Regdoc_graphs/'+idoc+'.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "    \n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "    \n",
    "    t0=0\n",
    "    while(t0<len(tokens1)):\n",
    "        if('\"edges\": {' in tokens1[t0]):\n",
    "            break\n",
    "        t0+=1\n",
    "        \n",
    "    with open(\"nodefont.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<t0):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = t0\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "    \n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open('./Regdoc_graphs/'+idoc+'.html', \"w\") as outf:\n",
    "        outf.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informal Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informalguidance(docs, finalclean48):\n",
    "    \n",
    "    with open('ig.pickle','rb') as f:\n",
    "        igdata = pickle.load(f)\n",
    "        \n",
    "    flag=0\n",
    "    iggraph = []\n",
    "    iglabel = []\n",
    "    igdocs = []\n",
    "    for i in igdata:\n",
    "        regindex = i[1].split()[1]\n",
    "        if(len(regindex) < 7 and ',' not in regindex):\n",
    "            if('(' not in regindex and ')' in regindex):\n",
    "                regindex = regindex.replace(')','')\n",
    "\n",
    "            subt=0\n",
    "            while(subt<len(regindex)):\n",
    "                if(regindex[subt] == '('):\n",
    "                    p=subt\n",
    "                    siter=0\n",
    "                    s=''\n",
    "                    while(siter<p):\n",
    "                        s = s + regindex[siter]\n",
    "                        siter+=1\n",
    "                    ri = s\n",
    "                    s = ''\n",
    "                    while(p<len(regindex)):\n",
    "                        if(regindex[p]!=')' and regindex[p].isdigit() == True):\n",
    "                            s = s + regindex[p]\n",
    "                        p+=1\n",
    "                    srid = s\n",
    "                    break\n",
    "                subt+=1\n",
    "            if(srid == ''):\n",
    "                srid = 'nil'\n",
    "        rid = [ri,srid]\n",
    "    \n",
    "        dind = docs.index(i[0])\n",
    "        cur_regs = finalclean48[dind]\n",
    "        regind = regids(cur_regs)\n",
    "    \n",
    "        iditer = 0\n",
    "        while(iditer<len(regind)):\n",
    "            if(rid[1] != 'nil'):\n",
    "                if(rid == regind[iditer]):\n",
    "                    iggraph.append([cur_regs[iditer],i[2],'3'])\n",
    "                    lbl = ''\n",
    "                    for li in i[3]:\n",
    "                        lbl = lbl + ' ' + li\n",
    "                    iglabel.append(lbl)\n",
    "                    igdocs.append(i[0])\n",
    "            else:\n",
    "                if(rid[0] == regind[0]):\n",
    "                    iggraph.append([cur_regs[iditer],i[2],'3'])\n",
    "                    lbl = ''\n",
    "                    for li in i[3]:\n",
    "                        lbl = lbl + ' ' + li\n",
    "                    iglabel.append(lbl)\n",
    "                    igdocs.append(i[0])\n",
    "            iditer+=1\n",
    "    return iggraph, iglabel, igdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legal Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legalcases(docs, finalclean48):\n",
    "    \n",
    "    with open('lc.pickle','rb') as f:\n",
    "        igdata = pickle.load(f)\n",
    "        \n",
    "    flag=0\n",
    "    iggraph = []\n",
    "    iglabel = []\n",
    "    igdocs = []\n",
    "    for i in igdata:\n",
    "        regindex = i[1].split()[1]\n",
    "        if(len(regindex) < 7 and ',' not in regindex):\n",
    "            if('(' not in regindex and ')' in regindex):\n",
    "                regindex = regindex.replace(')','')\n",
    "\n",
    "            subt=0\n",
    "            while(subt<len(regindex)):\n",
    "                if(regindex[subt] == '('):\n",
    "                    p=subt\n",
    "                    siter=0\n",
    "                    s=''\n",
    "                    while(siter<p):\n",
    "                        s = s + regindex[siter]\n",
    "                        siter+=1\n",
    "                    ri = s\n",
    "                    s = ''\n",
    "                    while(p<len(regindex)):\n",
    "                        if(regindex[p]!=')' and regindex[p].isdigit() == True):\n",
    "                            s = s + regindex[p]\n",
    "                        p+=1\n",
    "                    srid = s\n",
    "                    break\n",
    "                subt+=1\n",
    "            if(srid == ''):\n",
    "                srid = 'nil'\n",
    "        rid = [ri,srid]\n",
    "    \n",
    "        dind = docs.index(i[0])\n",
    "        cur_regs = finalclean48[dind]\n",
    "        regind = regids(cur_regs)\n",
    "    \n",
    "        iditer = 0\n",
    "        while(iditer<len(regind)):\n",
    "            if(rid[1] != 'nil'):\n",
    "                if(rid == regind[iditer]):\n",
    "                    iggraph.append([cur_regs[iditer],str(i[2]),'4'])\n",
    "                    lbl = ''\n",
    "                    for li in i[3]:\n",
    "                        lbl = lbl + ' ' + li\n",
    "                    iglabel.append(lbl)\n",
    "                    igdocs.append(i[0])\n",
    "            else:\n",
    "                if(rid[0] == regind[0]):\n",
    "                    iggraph.append([cur_regs[iditer],str(i[2]),'4'])\n",
    "                    lbl = ''\n",
    "                    for li in i[3]:\n",
    "                        lbl = lbl + ' ' + li\n",
    "                    iglabel.append(lbl)\n",
    "                    igdocs.append(i[0])\n",
    "            iditer+=1\n",
    "    lcgraph = iggraph\n",
    "    lclabel = iglabel\n",
    "    lcdocs = igdocs\n",
    "    return lcgraph, lclabel, lcdocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "iggraph, iglabel, igdocs = informalguidance(docs, finalclean48)\n",
    "lcgraph, lclabel, lcdocs = legalcases(docs, finalclean48)\n",
    "t=0\n",
    "topiccount = 0\n",
    "for idoc in docs:\n",
    "    cur_regs = finalclean48[t]\n",
    "    regdata = []\n",
    "    regind = regids(cur_regs)\n",
    "    regdata = crossref(cur_regs, regind, regdata)\n",
    "    igl = []\n",
    "    lcl = []\n",
    "    \n",
    "    d = docchaps[t]\n",
    "    regdata = chapterconnect(cur_regs, d, regdata)\n",
    "\n",
    "    topics = regtopics[topiccount:topiccount+len(cur_regs)]\n",
    "    topiccount = topiccount + len(cur_regs)\n",
    "    regdata = commontopics(topics,cur_regs,regdata)\n",
    "    \n",
    "    diter=0\n",
    "    while(diter<len(igdocs)):\n",
    "        if(igdocs[diter] == idoc):\n",
    "            regdata.append(iggraph[diter])\n",
    "            igl.append(iglabel[diter])\n",
    "        diter+=1\n",
    "\n",
    "    diter=0\n",
    "    while(diter<len(lcdocs)):\n",
    "        if(lcdocs[diter] == idoc):\n",
    "            regdata.append(lcgraph[diter])\n",
    "            lcl.append(lclabel[diter])\n",
    "        diter+=1\n",
    "    \n",
    "    df = pd.DataFrame(regdata, columns=['source','target','label'])\n",
    "    #if(idoc != 'Substantial Acquisition of Shares and Takeovers'):\n",
    "    layer21(df, idoc, regind, igl, lcl, t, cur_regs)\n",
    "    t=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(igdocs))\n",
    "print(len(lcdocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2 (All)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YET TO DO: Submitted shared topics data to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer22(df, finalclean48,iglabel):\n",
    "\n",
    "    sources = df['source']\n",
    "    targets = df['target']\n",
    "    labels = df['label']\n",
    "    q = 'Layer 2 - All Regulations'\n",
    "    edge_data = zip(sources, targets, labels)\n",
    "    igcount=0\n",
    "    lccount=0\n",
    "    #nt = Network(height=\"750px\", width=\"100%\", heading=q)\n",
    "    nt = Network(height=\"100%\", width=\"100%\",heading=q)\n",
    "    nt.hrepulsion()\n",
    "    \n",
    "    lbl = ['Cross-reference', 'Chapterwise', 'Shared topics']\n",
    "    t=0\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        h = e[2]\n",
    "        for i in finalclean48:\n",
    "            if(src in i):\n",
    "                sdocnumber = finalclean48.index(i)\n",
    "                scur_regs = i\n",
    "            if(dst in i):\n",
    "                ddocnumber = finalclean48.index(i)\n",
    "                dcur_regs = i\n",
    "        if(h == '0'):\n",
    "            sid = scur_regs.index(src)\n",
    "            did = dcur_regs.index(dst)\n",
    "            nt.add_node(src, label='sub-reg', title=src, color = \"blue\", url='./Reg_graphs/'+str(sdocnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label='sub-reg', title=dst, color = \"blue\", url='./Reg_graphs/'+str(ddocnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_edge(src, dst, title=lbl[0], value=8, color = \"#000000\", length=400)\n",
    "        elif(h == '1'):\n",
    "            sid = scur_regs.index(src)\n",
    "            did = dcur_regs.index(dst)\n",
    "            nt.add_node(src, label='sub-reg', title=src, color = \"blue\", url='./Reg_graphs/'+str(sdocnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label='sub-reg', title=dst, color = \"blue\", url='./Reg_graphs/'+str(ddocnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_edge(src, dst, title=lbl[1], value=7, color = \"red\", length=400)\n",
    "        elif(h == '2'):\n",
    "            sid = scur_regs.index(src)\n",
    "            did = dcur_regs.index(dst)\n",
    "            nt.add_node(src, label='sub-reg', title=src, color = \"blue\", url='./Reg_graphs/'+str(sdocnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label='sub-reg', title=dst, color = \"blue\", url='./Reg_graphs/'+str(ddocnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_edge(src, dst, title=lbl[2], color = \"green\", length=400)\n",
    "        elif(h == '3'):\n",
    "            sid = cur_regs.index(src)\n",
    "            nt.add_node(src, label=str(docnumber)+'_'+str(sid), title=src, color = \"blue\", url='./Reg_graphs/'+str(docnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label='informal_guidance', title=dst, color = \"red\", url='file:///home/deepti-saravanan/Desktop/knowledge_graph_SEBI/informal_guidance/'+dst.replace('.docx','')+'.pdf')\n",
    "            nt.add_edge(src, dst, title=igl[igcount], color = \"#000000\", length=400)\n",
    "            igcount = igcount+1\n",
    "            if(igcount == 657):\n",
    "                igcount=656\n",
    "        else:\n",
    "            sid = cur_regs.index(src)\n",
    "            nt.add_node(src, label=str(docnumber)+'_'+str(sid), title=src, color = \"blue\", url='/home/deepti-saravanan/Desktop/IIITH/variational_text_inference/Reg_graphs/'+str(docnumber)+'_'+str(sid)+'.html')\n",
    "            nt.add_node(dst, label='legal_case', title=dst, color = \"black\", url='file:///home/deepti-saravanan/Desktop/knowledge_graph_SEBI/case_pdf/'+dst+'.pdf')\n",
    "            nt.add_edge(src, dst, title=lcl[lccount], color = \"#000000\", length=400)\n",
    "            lccount = lccount+1\n",
    "        t=t+1\n",
    "    \n",
    "    nt.show_buttons()  \n",
    "    nt.write_html('./Regdoc_graphs/all.html')\n",
    "    \n",
    "    with open('./Regdoc_graphs/all.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "\n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "\n",
    "    t=0\n",
    "    appendind = []\n",
    "    for i in tokens1:\n",
    "        if('return' in i):\n",
    "            appendind.append(t)\n",
    "        t=t+1\n",
    "    \n",
    "    with open(\"interactive.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<appendind[-1]):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = appendind[-1]\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "\n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open('./Regdoc_graphs/all.html', \"w\") as outf:\n",
    "        outf.write(str(soup))\n",
    "        \n",
    "    with open('./Regdoc_graphs/all.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "    \n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "    \n",
    "    t0=0\n",
    "    while(t0<len(tokens1)):\n",
    "        if('\"edges\": {' in tokens1[t0]):\n",
    "            break\n",
    "        t0+=1\n",
    "        \n",
    "    with open(\"nodefont.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<t0):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = t0\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "    \n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open('./Regdoc_graphs/all.html', \"w\") as outf:\n",
    "        outf.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-30d4f9d1f5a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreggraphdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miggraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreggraphdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlayer22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinalclean48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miglabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-34df60d2194c>\u001b[0m in \u001b[0;36mlayer22\u001b[0;34m(df, finalclean48, iglabel)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sub-reg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./Reg_graphs/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdocnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sub-reg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./Reg_graphs/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddocnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlbl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_regs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.3/lib/python3.7/site-packages/pyvis/network.py\u001b[0m in \u001b[0;36madd_edge\u001b[0;34m(self, source, to, **options)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0mfrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m                 \u001b[0mdest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'to'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iggraph, iglabel, igdocs = informalguidance(docs, finalclean48)\n",
    "reggraphdata.extend(iggraph)\n",
    "df = pd.DataFrame(reggraphdata, columns=['source','target','label'])\n",
    "layer22(df,finalclean48,iglabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "with open('48annotated.pkl','rb') as f:\n",
    "    regtags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indreg(regtags, finalclean48):\n",
    "    d=41\n",
    "    while(d<len(finalclean48)):\n",
    "        print(d)\n",
    "        regz = finalclean48[d]\n",
    "        reg = list(regtags[d].keys())\n",
    "        labels = list(regtags[d].values())\n",
    "        \n",
    "        li=0\n",
    "        while(li<len(labels)):\n",
    "            l = labels[li]\n",
    "            r = reg[li]\n",
    "            \n",
    "            wordlabels1 = []\n",
    "            for tag in l:\n",
    "                k  = tag['start']\n",
    "                w = ''\n",
    "                while(k<tag['end']):\n",
    "                    w = w + r[k]\n",
    "                    k=k+1\n",
    "                wordlabels1.append([w,tag['label']])\n",
    "            \n",
    "            for j in wordlabels1:\n",
    "                if(j[1] == 'ADJ'):\n",
    "                    if(j[0] != 'such'):\n",
    "                        wordlabels1.remove(j)\n",
    "            \n",
    "            for j in wordlabels1:\n",
    "                t=0\n",
    "                if(t!=0):\n",
    "                    if(j[1] == 'VERB'):\n",
    "                        prevword = wordlabels1[t-1]\n",
    "                        if(j[0] in prevword[0]):\n",
    "                            wordlabels1.remove(j)\n",
    "                t=t+1\n",
    "            w = [wordlabels1]\n",
    "            \n",
    "            rsource = ['empty']\n",
    "            rtarget = ['empty']\n",
    "            rlabel = ['empty']\n",
    "            nodes = ['Person - General','Object','Person In charge','time','subject','PER','Assets','Investigation','Legal Doc','Company','ORG','Penalty']\n",
    "            cpt=0\n",
    "            for wordlabels in w:\n",
    "                k=0\n",
    "                p=-1\n",
    "                samecounter = 0\n",
    "                while(k<len(wordlabels)):\n",
    "                    if(k>p):\n",
    "                        i = wordlabels[k]\n",
    "            \n",
    "                        #Non-noun words\n",
    "            \n",
    "                        if(i[1] not in nodes):\n",
    "                \n",
    "                            #Source node identification\n",
    "                \n",
    "                            t = len(rtarget)\n",
    "                            s = rtarget[t-1]\n",
    "                            if((i[0] == 'or' or i[0] == 'and') and k+1 < len(wordlabels)):\n",
    "                                after = wordlabels[k+1]\n",
    "                                if(after[1] in nodes):\n",
    "                                    s = rsource[-1]\n",
    "                                else:\n",
    "                                    s = rsource[-1] \n",
    "                            rsource.append(s)\n",
    "                            samecounter = samecounter+1\n",
    "                \n",
    "                            #Target node and edge label identification\n",
    "                \n",
    "                            p=k\n",
    "                            l = ''\n",
    "                            while(p<len(wordlabels)):\n",
    "                                j = wordlabels[p]\n",
    "                                if(j[1] not in nodes):\n",
    "                                    if(j[0] != 'such'):\n",
    "                                        l = l + ' ' + j[0]\n",
    "                                    if(j[0] == 'or' or j[0] == 'and'):\n",
    "                                        prev = wordlabels[p-1]\n",
    "                                        if(prev[1] in nodes):\n",
    "                                            l = rlabel[-1]\n",
    "                                    if(j[0] == 'same'):\n",
    "                                        samewords = []\n",
    "                                        rs = rsource[len(rsource)-samecounter-1:]\n",
    "                                        for stoken in rs:\n",
    "                                            for token in wordlabels:\n",
    "                                                if(token[0] == stoken):\n",
    "                                                    if(token[1] == 'Object'):\n",
    "                                                         samewords.append(token[0])\n",
    "                                            if(len(samewords) != 0):\n",
    "                                                rlabel.append(l)\n",
    "                                                rtarget.append(samewords[0]) \n",
    "                                                break\n",
    "                                            else:\n",
    "                                                continue\n",
    "                            \n",
    "                                    if(j[0] == 'such' and p+1 < len(wordlabels)):\n",
    "                                        nextword = wordlabels[p+1]\n",
    "                                        for st in rsource:\n",
    "                                            if(nextword[0] in st):\n",
    "                                                targetword = st\n",
    "                                                break\n",
    "                                            else:\n",
    "                                                targetword = nextword[0]\n",
    "                                        rtarget.append(targetword)\n",
    "                                        rlabel.append(l)\n",
    "                            \n",
    "                                        if(cpt==1):\n",
    "                                            s = rtarget[-3]\n",
    "                                            rsource.append(s)\n",
    "                                            rlabel.append(l)\n",
    "                                            rtarget.append(j[0])\n",
    "                                            cpt=0\n",
    "                            \n",
    "                                        prev = wordlabels[p-1]\n",
    "                                        if(prev[0] == 'or' or prev[0] == 'and'):\n",
    "                                            cpt=1\n",
    "                                        p=p+1\n",
    "                                        break\n",
    "                                else:\n",
    "                                    rtarget.append(j[0])\n",
    "                                    rlabel.append(l)\n",
    "                        \n",
    "                                    if(cpt==1):\n",
    "                                        s = rtarget[-3]\n",
    "                                        rsource.append(s)\n",
    "                                        rlabel.append(l)\n",
    "                                        rtarget.append(j[0])\n",
    "                                        cpt=0\n",
    "                        \n",
    "                                    prev = wordlabels[p-1]\n",
    "                                    if(prev[0] == 'or' or prev[0] == 'and'):\n",
    "                                        cpt=1\n",
    "                                    break\n",
    "                                p=p+1\n",
    "                    \n",
    "                        #Nouns\n",
    "            \n",
    "                        else:\n",
    "                \n",
    "                            #Source node identification\n",
    "                            if(k==0):\n",
    "                                prev = 'nil'\n",
    "                            else:\n",
    "                                prev = wordlabels[k-1]\n",
    "                            #print(prev)\n",
    "                            if(prev[1] in nodes):\n",
    "                                orloc = k\n",
    "                                if(orloc+5 < len(wordlabels)):\n",
    "                                    orloc1 = orloc+5\n",
    "                                else:\n",
    "                                    orloc1 = len(wordlabels)\n",
    "                                orwords = []\n",
    "                                t = len(rsource)\n",
    "                                s = rsource[t-1]\n",
    "                                l = rlabel[t-1]\n",
    "                                while(orloc < orloc1):\n",
    "                                    orloc2 = wordlabels[orloc]\n",
    "                                    #print(orloc2)\n",
    "                                    orwords.append(orloc2[0])\n",
    "                                    orloc+=1\n",
    "                                if('or' in orwords):\n",
    "                                    orind = orwords.index('or')\n",
    "                                    orloc = 0\n",
    "                                    k1 = k\n",
    "                                    while(orloc < orind):\n",
    "                                        if(k1 < len(wordlabels)-1):\n",
    "                                            orloc2 = wordlabels[k1]\n",
    "                                        else:\n",
    "                                            orloc2 = wordlabels[-1]\n",
    "                                        rsource.append(s)\n",
    "                                        samecounter = samecounter+1\n",
    "                                        rlabel.append(l)\n",
    "                                        rtarget.append(orloc2[0])\n",
    "                                        k1+=1\n",
    "                                        orloc+=1\n",
    "                                    rsource.append(s)\n",
    "                                    samecounter = samecounter+1\n",
    "                                    rlabel.append(l)\n",
    "                                    k1+=1\n",
    "                                    if(k1 < len(wordlabels)-1):\n",
    "                                        orloc2 = wordlabels[k1]\n",
    "                                    else:\n",
    "                                        orloc2 = wordlabels[-1]\n",
    "                                    rtarget.append(orloc2[0])\n",
    "                                    p = k1+1\n",
    "                        \n",
    "                                elif('and' in orwords):\n",
    "                                    orind = orwords.index('and')\n",
    "                                    orloc = k\n",
    "                                    while(orloc < orind):\n",
    "                                        orloc2 = wordlabels[orloc]\n",
    "                                        rsource.append(s)\n",
    "                                        samecounter = samecounter+1\n",
    "                                        rlabel.append(l)\n",
    "                                        rtarget.append(orloc2[0])\n",
    "                                        orloc+=1\n",
    "                                    rsource.append(s)\n",
    "                                    samecounter = samecounter+1\n",
    "                                    rlabel.append(l)\n",
    "                                    orloc+=1\n",
    "                                    orloc2 = wordlabels[orloc]\n",
    "                                    rtarget.append(orloc2[0])\n",
    "                                    p = orloc\n",
    "                            \n",
    "                                else:\n",
    "                                    l = ''\n",
    "                                    t = len(rsource)\n",
    "                                    s = rtarget[t-1]\n",
    "                                    rsource.append(s)\n",
    "                                    samecounter = samecounter+1\n",
    "                                    rtarget.append(i[0])\n",
    "                                    rlabel.append('nil')\n",
    "                            else:\n",
    "                                l = ''\n",
    "                                if(k==0):\n",
    "                                    s1 = wordlabels[k]\n",
    "                                    s=s1[0]\n",
    "                                else:\n",
    "                                    t = len(rsource)\n",
    "                                    s = rtarget[t-1]\n",
    "                                rsource.append(s)\n",
    "                                samecounter = samecounter+1\n",
    "                \n",
    "                                if(k==0):\n",
    "                                    p=k+1\n",
    "                                else:\n",
    "                                    p=k\n",
    "                                while(p<len(wordlabels)):\n",
    "                                    j = wordlabels[p]\n",
    "                                    if(j[1] not in nodes):\n",
    "                                        if(j[0] != 'such'):\n",
    "                                            l = l + ' ' + j[0]\n",
    "                                        if(j[0] == 'or' or j[0] == 'and'):\n",
    "                                            prev = wordlabels[p-1]\n",
    "                                            if(prev[1] in nodes):\n",
    "                                                l = rlabel[-1]\n",
    "                                        if(j[0] == 'same'):\n",
    "                                            samewords = []\n",
    "                                            rs = rsource[len(rsource)-samecounter-1:]\n",
    "                            \n",
    "                                            for stoken in rs:\n",
    "                                                for token in wordlabels:\n",
    "                                                    if(token[0] == stoken):\n",
    "                                                        if(token[1] == 'Object'):\n",
    "                                                             samewords.append(token[0])\n",
    "                                            if(len(samewords) != 0):\n",
    "                                                rlabel.append(l)\n",
    "                                                rtarget.append(samewords[0]) \n",
    "                                                break\n",
    "                                            else:\n",
    "                                                continue\n",
    "                                        if(j[0] == 'such' and p+1 < len(wordlabels)):\n",
    "                                            nextword = wordlabels[p+1]\n",
    "                                            for st in rsource:\n",
    "                                                if(nextword[0] in st):\n",
    "                                                    targetword = st\n",
    "                                                    break\n",
    "                                                else:\n",
    "                                                    targetword = nextword[0]\n",
    "                                            rtarget.append(targetword)\n",
    "                                            rlabel.append(l)\n",
    "                            \n",
    "                                            if(cpt==1):\n",
    "                                                s = rtarget[-3]\n",
    "                                                rsource.append(s)\n",
    "                                                samecounter = samecounter+1\n",
    "                                                rlabel.append(l)\n",
    "                                                rtarget.append(j[0])\n",
    "                                                cpt=0\n",
    "                            \n",
    "                                            prev = wordlabels[p-1]\n",
    "                                            if(prev[0] == 'or' or prev[0] == 'and'):\n",
    "                                                cpt=1\n",
    "                                            p=p+1\n",
    "                                            break\n",
    "                                    else:\n",
    "                                        rtarget.append(j[0])\n",
    "                                        rlabel.append(l)\n",
    "                        \n",
    "                                        if(cpt==1):\n",
    "                                            s = rtarget[-3]\n",
    "                                            rsource.append(s)\n",
    "                                            samecounter = samecounter+1\n",
    "                                            rlabel.append(l)\n",
    "                                            rtarget.append(j[0])\n",
    "                                            cpt=0\n",
    "                            \n",
    "                                        prev = wordlabels[p-1]\n",
    "                                        if(prev[0] == 'or' or prev[0] == 'and'):\n",
    "                                            cpt=1\n",
    "                                        break\n",
    "                                    p=p+1\n",
    "                        if(len(rsource) != len(rtarget)):\n",
    "                            del rsource[len(rsource)-1]\n",
    "                    k=k+1\n",
    "                   \n",
    "            ingreggraph = []       \n",
    "            if(rlabel[0] == 'empty'):\n",
    "                rlabel.remove('empty')\n",
    "                rtarget.remove('empty')\n",
    "                rsource.pop(0)   \n",
    "            \n",
    "            regiter = 0\n",
    "            while(regiter<len(rsource)):\n",
    "                ingreggraph.append([rsource[regiter], rtarget[regiter], rlabel[regiter]])\n",
    "                regiter+=1\n",
    "            df = pd.DataFrame(ingreggraph, columns=['source','target','label'])\n",
    "            \n",
    "            layer3(df,regz[li],d,li)\n",
    "            #print(li)\n",
    "            li+=1\n",
    "        d+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3(df, reg, d,li):\n",
    "    sources = df['source']\n",
    "    targets = df['target']\n",
    "    labels = df['label']\n",
    "    q = 'Layer 3 - ' + reg\n",
    "    edge_data = zip(sources, targets, labels)\n",
    "    \n",
    "    #nt = Network(height=\"750px\", width=\"100%\", heading=q)\n",
    "    nt = Network(height=\"100%\", width=\"100%\",heading=q, directed=True)\n",
    "    nt.hrepulsion()\n",
    "    t=0\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        lbl = e[2]\n",
    "        if(t==0):\n",
    "            nt.add_node(src, label=src, title=src, color = \"red\")\n",
    "            nt.add_node(dst, label=dst, title=dst, color = \"blue\")\n",
    "            nt.add_edge(src, dst, title=lbl, color = \"#000000\")\n",
    "        else:\n",
    "            nt.add_node(src, label=src, title=src, color = \"blue\")\n",
    "            nt.add_node(dst, label=dst, title=dst, color = \"blue\")\n",
    "            nt.add_edge(src, dst, title=lbl, color = \"#000000\")\n",
    "        t=t+1\n",
    "    nt.show_buttons()  \n",
    "    nt.write_html('./Reg_graphs/'+str(d)+'_'+str(li)+'.html')\n",
    "        \n",
    "    with open('./Reg_graphs/'+str(d)+'_'+str(li)+'.html') as inf:\n",
    "        txt = inf.read()\n",
    "        soup = bs4.BeautifulSoup(txt)  \n",
    "    c = soup.find_all('script',type=\"text/javascript\")\n",
    "    tag = c[-1]\n",
    "    \n",
    "    t=0\n",
    "    start = 0\n",
    "    tindex = []\n",
    "    tokens1 = tag.contents[0].splitlines(True)\n",
    "    \n",
    "    t0=0\n",
    "    while(t0<len(tokens1)):\n",
    "        if('\"edges\": {' in tokens1[t0]):\n",
    "            break\n",
    "        t0+=1\n",
    "        \n",
    "    with open(\"/home/deepti-saravanan/Desktop/laws/nodefont.js\") as f:\n",
    "        jstext = f.read()\n",
    "    \n",
    "    p = []\n",
    "    t=0\n",
    "    while(t<t0):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    p.append(jstext)\n",
    "    t = t0\n",
    "    while(t<len(tokens1)):\n",
    "        p.append(tokens1[t])\n",
    "        t=t+1\n",
    "    \n",
    "    s = ''\n",
    "    for i in p:\n",
    "        if(i == '\\n'):\n",
    "            s = s + ' ' + '\\n'\n",
    "        else:\n",
    "            s = s + ' ' + i\n",
    "        \n",
    "    tag.string = s\n",
    "    \n",
    "    soup.find_all('script',type=\"text/javascript\")[-1] = tag\n",
    "    with open('./Reg_graphs/'+str(d)+'_'+str(li)+'.html', \"w\") as outf:\n",
    "        outf.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "indreg(regtags, finalclean48)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "compbio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
